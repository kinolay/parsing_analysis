# parsing_analysis

В файле "Downloaded_data.csv" находятся данные до обработки, за период 19.09.2023-25.09.2023

В файле "work_with_data.csv" находятся данные после обработки и после разбиение столбца улица на отдельные адреса

1) Файл "parsing": считывание таблицы с сайта https://rosseti-lenenergo.ru/planned_work/, фильтр по времени - период текущей недели (текущий день + 6 дней), заметил что данные отсортированы по дате (от будующих к прошедшим) и чтобы экономить ресурсы сделал возможность выбрать предельную страницу  сбора данных. Настройку автоматического запуска скрипта по расписанию можно сделать с помощью планировщика задач в linux либо через бесконечный цикл в коде, и модуль schedule. Но для того чтобы экономить ресурсы и иметь возможность делать правки в коде, лучше сделать через планировщик задач. В командной строке написать "crontab -e", добавить следующую строку в файл crontab: 0 9 * * 2 /usr/bin/python3/путь_к_вашему_скрипту/parsing.py. И теперь парсинг будет осуществляться каждый вторник в 9:00
 
2) Работа с данными, файл "work_with_data": разбиение столбца улица на отдельные адреса. Геокод адресов через https://petersburg.ru/mainPortal/api_services/view/2223 и сохранение building_id найденных зданий. Результат записан в csv файл
     
3) Анализ и визуализация данных: построил графики по полученным данным в yandex datalens. Ссылка на дашборд: datalens.yandex/182n1dyksc5cq

